{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Alshubaily_Ibrahim_Point Cloud Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ap2NvZ1R3pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import glob\n",
        "\n",
        "import math\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import urllib\n",
        "\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def parse_txt_array(src, sep=None, start=0, end=None, dtype=None, device=None):\n",
        "    src = [[float(x) for x in line.split(sep)[start:end]] for line in src]\n",
        "    src = torch.tensor(src, dtype=dtype).squeeze()\n",
        "    return src\n",
        "\n",
        "\n",
        "def read_txt_array(path, sep=None, start=0, end=None, dtype=None, device=None):\n",
        "    with open(path, 'r') as f:\n",
        "        src = f.read().split('\\n')[:-1]\n",
        "    return parse_txt_array(src, sep, start, end, dtype, device)\n",
        "\n",
        "\n",
        "def parse_off(src):\n",
        "    if src[0] == 'OFF':\n",
        "        src = src[1:]\n",
        "    else:\n",
        "        src[0] = src[0][3:]\n",
        "\n",
        "    num_nodes, num_faces = [int(item) for item in src[0].split()[:2]]\n",
        "\n",
        "    pos = parse_txt_array(src[1:1 + num_nodes])\n",
        "\n",
        "    face = src[1 + num_nodes:1 + num_nodes + num_faces]\n",
        "    face = face_to_tri(face)\n",
        "\n",
        "    return pos, face\n",
        "\n",
        "\n",
        "def face_to_tri(face):\n",
        "    face = [[int(x) for x in line.strip().split(' ')] for line in face]\n",
        "\n",
        "    triangle = torch.tensor([line[1:] for line in face if line[0] == 3])\n",
        "    triangle = triangle.to(torch.int64)\n",
        "\n",
        "    rect = torch.tensor([line[1:] for line in face if line[0] == 4])\n",
        "    rect = rect.to(torch.int64)\n",
        "\n",
        "    if rect.numel() > 0:\n",
        "        first, second = rect[:, [0, 1, 2]], rect[:, [1, 2, 3]]\n",
        "        return torch.cat([triangle, first, second], dim=0).t().contiguous()\n",
        "    else:\n",
        "        return triangle.t().contiguous()\n",
        "\n",
        "\n",
        "def read_off(path):\n",
        "    with open(path, 'r') as f:\n",
        "        src = f.read().split('\\n')[:-1]\n",
        "    return parse_off(src)\n",
        "\n",
        "\n",
        "def sample_points(pos, face, num=1024):\n",
        "    assert pos.size(1) == 3 and face.size(0) == 3\n",
        "\n",
        "    pos_max = pos.max()\n",
        "    pos = pos / pos_max\n",
        "\n",
        "    area = (pos[face[1]] - pos[face[0]]).cross(pos[face[2]] - pos[face[0]])\n",
        "    area = area.norm(p=2, dim=1).abs() / 2\n",
        "\n",
        "    prob = area / area.sum()\n",
        "    sample = torch.multinomial(prob, num, replacement=True)\n",
        "    face = face[:, sample]\n",
        "\n",
        "    frac = torch.rand(num, 2, device=pos.device)\n",
        "    mask = frac.sum(dim=-1) > 1\n",
        "    frac[mask] = 1 - frac[mask]\n",
        "\n",
        "    vec1 = pos[face[1]] - pos[face[0]]\n",
        "    vec2 = pos[face[2]] - pos[face[0]]\n",
        "\n",
        "    pos_sampled = pos[face[0]]\n",
        "    pos_sampled += frac[:, :1] * vec1\n",
        "    pos_sampled += frac[:, 1:] * vec2\n",
        "\n",
        "    pos_sampled = pos_sampled * pos_max\n",
        "\n",
        "    return pos_sampled\n",
        "\n",
        "\n",
        "class ModelNet(torch.utils.data.Dataset):\n",
        "    url = 'http://vision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip'\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        super(ModelNet, self).__init__()\n",
        "        self.root = root\n",
        "        self.raw_dir = osp.join(self.root, 'raw')\n",
        "        self.processed_dir = osp.join(self.root, 'processed')\n",
        "        self.transform = transform\n",
        "\n",
        "        self.download()\n",
        "        self.process()\n",
        "\n",
        "        path = self.processed_paths[0] if train else self.processed_paths[1]\n",
        "        self.data, self.targets = torch.load(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pos, target = self.data[idx], int(self.targets[idx])\n",
        "        if self.transform is not None:\n",
        "            pos = self.transform(pos)\n",
        "        return pos, target\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['training.pt', 'test.pt']\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [\n",
        "            'bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor',\n",
        "            'night_stand', 'sofa', 'table', 'toilet'\n",
        "        ]\n",
        "\n",
        "    @property\n",
        "    def raw_paths(self):\n",
        "        files = self.raw_file_names\n",
        "        return [osp.join(self.raw_dir, f) for f in files]\n",
        "\n",
        "    @property\n",
        "    def processed_paths(self):\n",
        "        files = self.processed_file_names\n",
        "        return [osp.join(self.processed_dir, f) for f in files]\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return int(self.targets.max().item()) + 1 \n",
        "\n",
        "    def download(self):\n",
        "\n",
        "        if all([osp.exists(f) for f in self.raw_paths]):\n",
        "            return\n",
        "\n",
        "        os.makedirs(osp.expanduser(osp.normpath(self.raw_dir)))\n",
        "\n",
        "        filename = self.url.rpartition('/')[2]\n",
        "        path = osp.join(self.root, filename)\n",
        "        if osp.exists(path):\n",
        "            print('Using exist file', filename)\n",
        "        else:\n",
        "            print('Downloading', self.url)\n",
        "            data = urllib.request.urlopen(self.url)\n",
        "            with open(path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "\n",
        "        with zipfile.ZipFile(path, 'r') as f:\n",
        "            print('Extracting', path)\n",
        "            f.extractall(self.root)\n",
        "        os.unlink(path)\n",
        "\n",
        "        folder = osp.join(self.root, 'ModelNet10')\n",
        "        shutil.rmtree(self.raw_dir)\n",
        "        os.rename(folder, self.raw_dir)\n",
        "        print('Done!')\n",
        "\n",
        "    def process(self):\n",
        "        if all([osp.exists(f) for f in self.processed_paths]):\n",
        "            return\n",
        "\n",
        "        print('Processing...')\n",
        "        os.makedirs(osp.expanduser(osp.normpath(self.processed_dir)))\n",
        "\n",
        "        self.process_set('train', self.processed_paths[0])\n",
        "        self.process_set('test', self.processed_paths[1])\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def process_set(self, dataset, processed_path):\n",
        "        categories = glob.glob(osp.join(self.raw_dir, '*', ''))\n",
        "        categories = sorted([x.split(os.sep)[-2] for x in categories])\n",
        "\n",
        "        positions = []\n",
        "        targets = []\n",
        "        for target, category in enumerate(categories):\n",
        "            folder = osp.join(self.raw_dir, category, dataset)\n",
        "            paths = glob.glob('{}/{}_*.off'.format(folder, category))\n",
        "\n",
        "            for path in paths:\n",
        "                pos, face = read_off(path)\n",
        "\n",
        "                scale = (1 / pos.abs().max()) * 0.999999\n",
        "                pos = pos * scale\n",
        "\n",
        "                pos = sample_points(pos, face)\n",
        "                positions.append(pos.t())\n",
        "                targets.append(target)\n",
        "\n",
        "        positions = torch.stack(positions)\n",
        "        targets = torch.Tensor(targets)\n",
        "\n",
        "        torch.save((positions, targets), processed_path)\n",
        "\n",
        "\n",
        "def fixed_points(pos, y, num):\n",
        "    N, D = pos.shape\n",
        "    assert D == 3\n",
        "    choice = torch.cat([torch.randperm(N)\n",
        "                        for _ in range(math.ceil(num / N))], dim=0)[:num]\n",
        "    return pos[choice], y[choice]\n",
        "\n",
        "class ShapeNet(torch.utils.data.Dataset):\n",
        "    url = 'https://shapenet.cs.stanford.edu/iccv17/partseg'\n",
        "\n",
        "    categories = {\n",
        "        'Airplane': '02691156',\n",
        "        'Bag': '02773838',\n",
        "        'Cap': '02954340',\n",
        "        'Car': '02958343',\n",
        "        'Chair': '03001627',\n",
        "        'Earphone': '03261776',\n",
        "        'Guitar': '03467517',\n",
        "        'Knife': '03624134',\n",
        "        'Lamp': '03636649',\n",
        "        'Laptop': '03642806',\n",
        "        'Motorbike': '03790512',\n",
        "        'Mug': '03797390',\n",
        "        'Pistol': '03948459',\n",
        "        'Rocket': '04099429',\n",
        "        'Skateboard': '04225987',\n",
        "        'Table': '04379243',\n",
        "    }\n",
        "\n",
        "    def __init__(self, root, category, train=True, transform=None):\n",
        "        super(ShapeNet, self).__init__()\n",
        "        self.category = category\n",
        "\n",
        "        assert self.category in self.categories\n",
        "\n",
        "        self.root = root\n",
        "        self.raw_dir = osp.join(self.root, 'raw')\n",
        "        self.processed_dir = osp.join(self.root, 'processed')\n",
        "        self.transform = transform\n",
        "\n",
        "        self.download()\n",
        "        self.process()\n",
        "\n",
        "        path = self.processed_paths[0] if train else self.processed_paths[1]\n",
        "        self.data, self.targets = torch.load(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pos, target = self.data[idx], self.targets[idx]\n",
        "        if self.transform is not None:\n",
        "            pos = self.transform(pos)\n",
        "        return pos, target\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [\n",
        "            'train_data', 'train_label', 'val_data', 'val_label', 'test_data',\n",
        "            'test_label'\n",
        "        ]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        names = ['training.pt', 'test.pt']\n",
        "        return [osp.join(self.category, name) for name in names]\n",
        "\n",
        "    @property\n",
        "    def raw_paths(self):\n",
        "        files = self.raw_file_names\n",
        "        return [osp.join(self.raw_dir, f) for f in files]\n",
        "\n",
        "    @property\n",
        "    def processed_paths(self):\n",
        "        files = self.processed_file_names\n",
        "        return [osp.join(self.processed_dir, f) for f in files]\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return int(self.targets.max().item()) + 1 \n",
        "\n",
        "    def download(self):\n",
        "        if all([osp.exists(f) for f in self.raw_paths]):\n",
        "            return\n",
        "        os.makedirs(osp.expanduser(osp.normpath(self.raw_dir)))\n",
        "\n",
        "        for name in self.raw_file_names:\n",
        "            url = '{}/{}.zip'.format(self.url, name)\n",
        "\n",
        "            filename = url.rpartition('/')[2]\n",
        "            path = osp.join(self.raw_dir, filename)\n",
        "            if osp.exists(path):\n",
        "                print('Using exist file', filename)\n",
        "            else:\n",
        "                print('Downloading', url)\n",
        "                data = urllib.request.urlopen(url)\n",
        "                with open(path, 'wb') as f:\n",
        "                    f.write(data.read())\n",
        "\n",
        "            with zipfile.ZipFile(path, 'r') as f:\n",
        "                print('Extracting', path)\n",
        "                f.extractall(self.raw_dir)\n",
        "            os.unlink(path)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def process(self):\n",
        "        if all([osp.exists(f) for f in self.processed_paths]):\n",
        "            return\n",
        "\n",
        "        print('Processing...')\n",
        "\n",
        "        directory = osp.expanduser(osp.normpath(\n",
        "            osp.join(self.processed_dir, self.category)))\n",
        "        if not osp.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        idx = self.categories[self.category]\n",
        "        paths = [osp.join(path, idx) for path in self.raw_paths]\n",
        "        datasets = []\n",
        "        for path in zip(paths[::2], paths[1::2]):\n",
        "            pos_paths = sorted(glob.glob(osp.join(path[0], '*.pts')))\n",
        "            y_paths = sorted(glob.glob(osp.join(path[1], '*.seg')))\n",
        "            positions, ys = [], []\n",
        "            for path in zip(pos_paths, y_paths):\n",
        "                pos = read_txt_array(path[0])\n",
        "                y = read_txt_array(path[1], dtype=torch.long)\n",
        "                pos, y = fixed_points(pos, y, 2048)\n",
        "\n",
        "                positions.append(pos.t())\n",
        "                ys.append(y)\n",
        "\n",
        "            positions = torch.stack(positions)\n",
        "            ys = torch.stack(ys)\n",
        "            datasets.append((positions, ys))\n",
        "\n",
        "        train_data = torch.cat([datasets[0][0], datasets[1][0]], dim=0), torch.cat(\n",
        "            [datasets[0][1], datasets[1][1]], dim=0)\n",
        "        test_data = datasets[2]\n",
        "\n",
        "        torch.save(train_data, self.processed_paths[0])\n",
        "        torch.save(test_data, self.processed_paths[1])\n",
        "\n",
        "        print('Done.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPLLFPki_0Gn",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "In this project, you will be asked to implement [PointNet](https://arxiv.org/abs/1612.00593) architecture and train a classification network (left) and a segmentation network (middle).\n",
        "![title](img/cls_sem.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLRl3YAh_0Gr",
        "colab_type": "text"
      },
      "source": [
        "### Grading Points\n",
        "* Task 1.1 - 5\n",
        "* Task 1.2 - 5\n",
        "* Task 2.1 - 10\n",
        "* Task 2.2 - 5\n",
        "* Task 2.3 - 5\n",
        "* Task 2.4 - 5\n",
        "* Task 2.5 - 5\n",
        "* Task 2.6 - 10\n",
        "* Task 2.7 - 5\n",
        "* Task 2.8 - 10\n",
        "* Task 2.9 - 10\n",
        "* Task 2.10 - 5 \n",
        "* Task 2.11 - 5\n",
        "* Task 2.12 - 5\n",
        "* Task 2.13 - 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn0gWDvw_0Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9kCwTx_0Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "#import dataset # custom dataset for ModelNet10 and ShapeNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS-_U-ND_0G5",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JENHig_0G6",
        "colab_type": "text"
      },
      "source": [
        "Usually, we write the point cloud as $X\\in\\mathbb{R}^{N\\times 3}$. While in programming, we use `B x 3 x N` layout, where `B` is the batch-size and `N` is the number of points in a single point cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCV5zkxq_0G6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Jitter the position of each points by a zero mean Gaussian\n",
        "For input $X\\in\\mathbb{R}^{N\\times 3}$, we transform $X$ by $X \\leftarrow X + \\mathcal{N}(0, \\sigma^2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Nc6-tg_0G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomJitter(object):\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        ## hint: useful function `torch.randn` and `torch.randn_like`\n",
        "        ## TASK 1.1\n",
        "        ## This function takes as input a point cloud of layout `3 x N`, \n",
        "        ## and output the jittered point cloud of layout `3 x N`.        \n",
        "        return data + (self.sigma * torch.randn_like(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXdSiKqT_0G9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## random generate data and test your transform here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY_qiC3e_0HA",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Rotate the object along the z-axis randomly\n",
        "For input $X\\in\\mathbb{R}^{N\\times 3}$, we rotate all points along z-axis (up-axis) by a degree $\\theta$.\n",
        "\n",
        "\n",
        "Suppose $T$ is the transformation matrix,\n",
        "$$X\\leftarrow XT,$$\n",
        "where $$T=\\begin{bmatrix}\\cos\\theta & \\sin\\theta & 0 \\\\ -\\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO-eb8nE_0HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "class RandomZRotate(object):\n",
        "    def __init__(self, degrees):\n",
        "        ## here `self.degrees` is a tuple (0, 360) which defines the range of degree\n",
        "        self.degrees = degrees\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        ## TASK 1.2\n",
        "        ## This function takes as input a point cloud of layout `3 x N`, \n",
        "        ## and output the rotated point cloud of layout `3 x N`.\n",
        "        ##\n",
        "        ## The rotation is along z-axis, and the degree is uniformly distributed\n",
        "        ## between [0, 360]\n",
        "        ##\n",
        "        ## hint: useful function `torch.randn`ï¼Œ `torch.randn_like` and `torch.matmul`\n",
        "        ##\n",
        "        ## Notice:   \n",
        "        ## Different from its math notation `N x 3`, the input has size of `3 x N`\n",
        "        d = np.random.uniform(self.degrees[0], self.degrees[1])\n",
        "        sin, cos = np.sin(d), np.cos(d)\n",
        "        rot_mat = torch.Tensor([[cos, sin, 0], \n",
        "                                [-sin, cos, 0], \n",
        "                                [0, 0, 1]])\n",
        "    \n",
        "        return torch.matmul(rot_mat, data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGW6t6P__0HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## random generate data and test your transform here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8oddvfW_0HD",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Load dataset ModelNet10 for Point Cloud Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ6UsKmY_0HE",
        "colab_type": "text"
      },
      "source": [
        "### ModelNet10\n",
        "By loading this dataset, we have data of shape `B x 3 x N` and label of shape `B`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUUjZ7Wu_0HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It may taske some time to download and pre-process the dataset.\n",
        "train_transform = Compose([RandomZRotate((0, 360)), RandomJitter(0.02)])\n",
        "train_cls_dataset = ModelNet(root='./ModelNet10', transform=train_transform, train=True)\n",
        "test_cls_dataset = ModelNet(root='./ModelNet10', train=False)\n",
        "train_cls_loader = data.DataLoader(\n",
        "    train_cls_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "test_cls_loader = data.DataLoader(\n",
        "    test_cls_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyznzv8P_0HF",
        "colab_type": "code",
        "outputId": "a61201d5-0492-4551-f78b-66b0dfa7b614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_cls_dataset.num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60_8tAMp_0HH",
        "colab_type": "text"
      },
      "source": [
        "## ShapeNet\n",
        "By loading this dataset, we have data of shape `B x 3 x N` and target of shape `B x N`.\n",
        "\n",
        "Here is the list of categories:\n",
        "['Airplane', 'Bag', 'Cap', 'Car', 'Chair', 'Earphone', 'Guitar', 'Knife', 'Lamp', 'Laptop', 'Motorbike', 'Mug', 'Pistol', 'Rocket', 'Skateboard', 'Table']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krnFxISb_0HH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Here as an example, we choose the cateogry 'Airplane'\n",
        "category = 'Airplane'\n",
        "train_seg_dataset = ShapeNet(root='./ShapeNet', category=category, train=True)\n",
        "test_seg_dataset = ShapeNet(root='./ShapeNet', category=category, train=False)\n",
        "train_seg_loader = data.DataLoader(\n",
        "    train_seg_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "test_seg_loader = data.DataLoader(\n",
        "    test_seg_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycsInDc__0HI",
        "colab_type": "code",
        "outputId": "d1ee2b5b-d634-431d-9836-11736b50f9da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_seg_dataset.num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3CxXgnX_0HJ",
        "colab_type": "text"
      },
      "source": [
        "# 2 PointNet Architecture (Read Section 4.2 and Appendix C)\n",
        "In this section, you will be asked to implement classification and segmentation step by step.\n",
        "![pointnet](img/pointnet.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxVDERbV_0HK",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Joint Alignment Network \n",
        "This mini-network takes as input matrix of size $N \\times K$, and outputs a transformation matrix of size $K \\times K$. \n",
        "\n",
        "In programming, the input size of this module is `B x K x N` and output size is `B x K x K`.\n",
        "\n",
        "For the shared MLP, use structure like this `(FC(64), BN, ReLU, FC(128), BN, ReLU, FC(1024), BN, ReLU)`.\n",
        "\n",
        "For the MLP after global max pooling, use structure like this `(FC(512), BN, ReLU, FC(256), BN, ReLU, FC(K*K)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8SxFeD_0HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformation(nn.Module):\n",
        "    def __init__(self, k=3):\n",
        "        super(Transformation, self).__init__()\n",
        "        \n",
        "        self.k = k\n",
        "        \n",
        "        ## TASK 2.1\n",
        "        \n",
        "        ## define your network layers here\n",
        "        ## shared mlp\n",
        "        ## input size: B x K x N\n",
        "        ## output size: B x 1024 x N\n",
        "        ## hint: you may want to use `nn.Conv1d` here. Why? translation invariant, (less peramtr = better effeiceny)\n",
        "        self.Shared_nn = nn.Sequential(\n",
        "            nn.Conv1d(k, 64, 1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(64, 128, 1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(128, 1024, 1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        ## define your network layers here\n",
        "        ## mlp\n",
        "        ## input size: B x 1024\n",
        "        ## output size: B x (K*K)\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, k * k))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, K, N = x.shape # batch-size, dim, number of points\n",
        "        ## TASK 2.1\n",
        "\n",
        "        ## forward of shared mlp\n",
        "        # input - B x K x N\n",
        "        # output - B x 1024 x N\n",
        "        x = self.Shared_nn(x)\n",
        "\n",
        "        ## global max pooling\n",
        "        # input - B x 1024 x N\n",
        "        # output - B x 1024\n",
        "        x = F.max_pool1d(x, N).view(B, 1024)\n",
        "\n",
        "        \n",
        "        ## mlp\n",
        "        # input - B x 1024\n",
        "        # output - B x (K*K)\n",
        "        x = self.nn(x)\n",
        "        \n",
        "        ## reshape the transformation matrix to B x K x K\n",
        "        identity = torch.eye(self.k, device=x.device)\n",
        "        x = x.view(B, self.k, self.k) + identity[None]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrHG7rIA_0HL",
        "colab_type": "code",
        "outputId": "2f17e925-4984-4517-c6f2-760649d08ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "Transformation()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformation(\n",
              "  (Shared_nn): Sequential(\n",
              "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "  )\n",
              "  (nn): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=256, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_fl_I4v_0HM",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Regularization Loss\n",
        "$$L_{reg}=\\|I-TT^\\intercal\\|^2_F$$\n",
        "\n",
        "The output of `Transformation` network is of size `B x K x K`. The module `OrthoLoss` has no trainable parameters, only computes this norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CpqIYGR_0HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OrthoLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OrthoLoss, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## hint: useful function `torch.bmm` and `torch.matmul`\n",
        "        \n",
        "        ## TASK 2.2\n",
        "        ## compute the matrix product\n",
        "        prod = torch.bmm(x, x.transpose(2,1))\n",
        "\n",
        "        norm = torch.norm(prod - torch.eye(x.shape[1], device=x.device)[None], dim=(1,2))\n",
        "        return norm.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlRaPQCq_0HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## random generate data and test this network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmmCCMO1_0HO",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Feature Network\n",
        "In this subsection, you will be asked to implement the feature network (the top branch).\n",
        "\n",
        "Local features are a matrix of size `B x 64 x N`, which will be used in the segmentation task.\n",
        "\n",
        "Global features are a matrix of size `B x 1024`, which will be used in the classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNDLvdph_0HO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Feature(nn.Module):\n",
        "    def __init__(self, alignment=False):\n",
        "        super(Feature, self).__init__()\n",
        "        \n",
        "        self.alignment = alignment\n",
        "        \n",
        "        ## `input_transform` calculates the input transform matrix of size `3 x 3`\n",
        "        if self.alignment:\n",
        "            self.input_transform = Transformation(3)\n",
        "        \n",
        "        ## TASK 2.3\n",
        "        ## define your network layers here\n",
        "        ## local feature\n",
        "        ## shared mlp\n",
        "        ## input size: B x 3 x N\n",
        "        ## output size: B x 64 x N\n",
        "        ## hint: you may want to use `nn.Conv1d` here.\n",
        "        self.local_nn = nn.Sequential( nn.Conv1d(3, 64, 1),\n",
        "                                       nn.BatchNorm1d(64),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "        \n",
        "        ## `feature_transform` calculates the feature transform matrix of size `64 x 64`\n",
        "        if self.alignment:\n",
        "            self.feature_transform = Transformation(64)\n",
        "        \n",
        "        ## TASK 2.4\n",
        "        ## define your network layers here\n",
        "        ## global feature\n",
        "        ## shared mlp\n",
        "        ## input size: B x 64 x N\n",
        "        ## output size: B x 1024 x N  \n",
        "        self.global_nn = nn.Sequential( nn.Conv1d(64, 128, 1),\n",
        "                                        nn.BatchNorm1d(128),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Conv1d(128, 1024, 1),\n",
        "                                        nn.BatchNorm1d(1024),\n",
        "                                        nn.ReLU(inplace=True))      \n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, K, N = x.shape\n",
        "        \n",
        "        ## apply the input transform\n",
        "        if self.alignment:\n",
        "            transform = self.input_transform(x)\n",
        "            ## TASK 2.5\n",
        "            ## apply the input transform\n",
        "            x = torch.bmm(x.transpose(2, 1), transform).transpose(2, 1)\n",
        "\n",
        "        ## TASK 2.3\n",
        "        ## forward of shared mlp\n",
        "        # input - B x K x N\n",
        "        # output - B x 64 x N\n",
        "        x = self.local_nn(x)\n",
        "        \n",
        "        if self.alignment:\n",
        "            transform = self.feature_transform(x)\n",
        "            ## TASK 2.5\n",
        "            ## apply the feature transform\n",
        "            x = torch.bmm(x.transpose(2, 1), transform).transpose(2, 1)\n",
        "        else:\n",
        "            ## do not modify this line\n",
        "            transform = None\n",
        "        \n",
        "        local_feature = x\n",
        "        \n",
        "        ## TASK 2.4\n",
        "        ## forward of shared mlp\n",
        "        # input - B x 64 x N\n",
        "        # output - B x 1024 x N\n",
        "        x = self.global_nn(x)\n",
        "        \n",
        "        ## TASK 2.4\n",
        "        ## global max pooling\n",
        "        # input - B x 1024 x N\n",
        "        # output - B x 1024\n",
        "        x = F.max_pool1d(x, kernel_size = N).view(B, 1024)\n",
        "        \n",
        "        global_feature = x\n",
        "        \n",
        "        ## summary:\n",
        "        ## global_feature: B x 1024\n",
        "        ## local_feature: B x 64 x N\n",
        "        ## transform: B x K x K\n",
        "        return global_feature, local_feature, transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-_Z-QVn_0HP",
        "colab_type": "code",
        "outputId": "1c46af90-0194-451e-9ece-72fe19f36fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "Feature()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Feature(\n",
              "  (local_nn): Sequential(\n",
              "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (global_nn): Sequential(\n",
              "    (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1deWuSkn_0HQ",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Classification Network\n",
        "In this network, you will use the global features generated by the `Feature` network defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1EthdP9_0HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classification(nn.Module):\n",
        "    def __init__(self, num_classes, alignment=False):\n",
        "        super(Classification, self).__init__()\n",
        "                \n",
        "        self.feature = Feature(alignment=alignment)\n",
        "        \n",
        "        ## TASK 2.6\n",
        "        ## define your network layers here\n",
        "        ## mlp\n",
        "        ## input size: B x 1024\n",
        "        ## output size: B x num_classes\n",
        "        self.mlp = nn.Sequential( nn.Linear(1024, 512),\n",
        "                                  nn.BatchNorm1d(512),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "                                  nn.Linear(512, 256),\n",
        "                                  nn.BatchNorm1d(256),\n",
        "                                  nn.ReLU(inplace=True),\n",
        "                                  nn.Dropout(0.7),\n",
        "                                  nn.Linear(256, num_classes),\n",
        "                                  nn.LogSoftmax(1))\n",
        "    def forward(self, x):\n",
        "        # x is the global feature matrix\n",
        "        # here we don't use local feature matrix\n",
        "        x, _, trans = self.feature(x)\n",
        "        \n",
        "        ## TASK 2.6\n",
        "        ## forward of mlp\n",
        "        # input - B x 1024\n",
        "        # output - B x num_classes        \n",
        "        x = self.mlp(x)\n",
        "        \n",
        "        ## x: B x num_classes\n",
        "        ## trans: B x K x K\n",
        "        return x, trans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om6hD2wZ_0HS",
        "colab_type": "code",
        "outputId": "000a2830-cbeb-48fb-c267-81df67a03347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "Classification(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(\n",
              "  (feature): Feature(\n",
              "    (local_nn): Sequential(\n",
              "      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (global_nn): Sequential(\n",
              "      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "      (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Dropout(p=0.7, inplace=False)\n",
              "    (7): Linear(in_features=256, out_features=10, bias=True)\n",
              "    (8): LogSoftmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6_mExBs_0HS",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1 Train this network on ModelNet10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwBrZ5S7_0HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main train function for classification\n",
        "def train_cls(train_loader, test_loader, network, optimizer, epochs, scheduler):\n",
        "    reg = OrthoLoss()\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
        "        print('Training...')\n",
        "        network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        for batch, (pos, label) in enumerate(train_loader):\n",
        "            network.zero_grad()\n",
        "            pos, label = pos.cuda(), label.cuda()\n",
        "            \n",
        "            ## TASK 2.7\n",
        "            ## forward propagation\n",
        "            output, trans = network(pos)\n",
        "            loss = F.nll_loss(output, label)\n",
        "            ##########\n",
        "            \n",
        "            ## regularizer\n",
        "            if trans is not None:\n",
        "                loss += reg(trans) * 0.001\n",
        "\n",
        "            pred = output.max(1)[1]\n",
        "            correct += pred.eq(label).sum().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
        "        \n",
        "        scheduler.step()\n",
        "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/len(train_loader.dataset) * 100))\n",
        "        \n",
        "        print('\\nTesting...')\n",
        "        with torch.no_grad():\n",
        "            network.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            for batch, (pos, label) in enumerate(test_loader):\n",
        "                pos, label = pos.cuda(), label.cuda()\n",
        "    \n",
        "                ## TASK 2.7\n",
        "                ## forward propagation\n",
        "                output, trans = network(pos)\n",
        "                loss = F.nll_loss(output, label)\n",
        "                ##########\n",
        "\n",
        "                if trans is not None:\n",
        "                    loss += reg(trans) * 0.001\n",
        "\n",
        "                pred = output.max(1)[1]\n",
        "                correct += pred.eq(label).sum().item()\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
        "\n",
        "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/len(test_loader.dataset) * 100))\n",
        "        print('-------------------------------------------')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdl5OMZ3_0HU",
        "colab_type": "code",
        "outputId": "a6ccd352-d423-4a8e-b494-a98c483b6f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network = Classification(10, alignment=True).cuda()\n",
        "epochs = 200 # you can change the value to a small number for debugging\n",
        "\n",
        "## TASK 2.8\n",
        "# see Appendix C\n",
        "# choose an optimizer and an initial learning rate\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
        "# choose a lr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.5)\n",
        "#######3\n",
        "\n",
        "# start training\n",
        "train_cls(train_cls_loader, test_cls_loader, network, optimizer, epochs, scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:[01/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.1645\n",
            "Average Train Loss: 1.6611; Train Acc: 49.7369\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.9946\n",
            "Average Test Loss: 1.4478; Test Acc: 53.3040\n",
            "-------------------------------------------\n",
            "Epoch:[02/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 2.2179\n",
            "Average Train Loss: 1.1377; Train Acc: 63.4929\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.6138\n",
            "Average Test Loss: 1.1410; Test Acc: 63.6564\n",
            "-------------------------------------------\n",
            "Epoch:[03/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3242\n",
            "Average Train Loss: 0.9861; Train Acc: 68.2786\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 2.2894\n",
            "Average Test Loss: 1.1798; Test Acc: 55.0661\n",
            "-------------------------------------------\n",
            "Epoch:[04/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 3.9719\n",
            "Average Train Loss: 0.9699; Train Acc: 69.8822\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.3378\n",
            "Average Test Loss: 1.0544; Test Acc: 62.0044\n",
            "-------------------------------------------\n",
            "Epoch:[05/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4802\n",
            "Average Train Loss: 0.8393; Train Acc: 72.7387\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1332\n",
            "Average Test Loss: 0.7565; Test Acc: 72.3568\n",
            "-------------------------------------------\n",
            "Epoch:[06/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.2196\n",
            "Average Train Loss: 0.7729; Train Acc: 76.0461\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.2021\n",
            "Average Test Loss: 0.7069; Test Acc: 77.0925\n",
            "-------------------------------------------\n",
            "Epoch:[07/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.6739\n",
            "Average Train Loss: 0.7233; Train Acc: 76.8479\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1235\n",
            "Average Test Loss: 0.6853; Test Acc: 75.2203\n",
            "-------------------------------------------\n",
            "Epoch:[08/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.5382\n",
            "Average Train Loss: 0.6915; Train Acc: 78.1258\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1700\n",
            "Average Test Loss: 0.8172; Test Acc: 70.3744\n",
            "-------------------------------------------\n",
            "Epoch:[09/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.9662\n",
            "Average Train Loss: 0.6494; Train Acc: 79.3535\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1515\n",
            "Average Test Loss: 0.6606; Test Acc: 80.3965\n",
            "-------------------------------------------\n",
            "Epoch:[10/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.7878\n",
            "Average Train Loss: 0.6231; Train Acc: 79.8547\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0397\n",
            "Average Test Loss: 0.6556; Test Acc: 80.1762\n",
            "-------------------------------------------\n",
            "Epoch:[11/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.4491\n",
            "Average Train Loss: 0.6017; Train Acc: 80.0551\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0305\n",
            "Average Test Loss: 0.5688; Test Acc: 77.2026\n",
            "-------------------------------------------\n",
            "Epoch:[12/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.5600\n",
            "Average Train Loss: 0.5816; Train Acc: 81.7840\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.2775\n",
            "Average Test Loss: 0.6658; Test Acc: 75.1101\n",
            "-------------------------------------------\n",
            "Epoch:[13/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.4021\n",
            "Average Train Loss: 0.5531; Train Acc: 82.5858\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0734\n",
            "Average Test Loss: 0.6054; Test Acc: 79.7357\n",
            "-------------------------------------------\n",
            "Epoch:[14/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 2.1741\n",
            "Average Train Loss: 0.5400; Train Acc: 83.4628\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1128\n",
            "Average Test Loss: 0.5382; Test Acc: 80.2863\n",
            "-------------------------------------------\n",
            "Epoch:[15/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.0505\n",
            "Average Train Loss: 0.5106; Train Acc: 83.7134\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0523\n",
            "Average Test Loss: 0.5159; Test Acc: 82.8194\n",
            "-------------------------------------------\n",
            "Epoch:[16/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1930\n",
            "Average Train Loss: 0.5170; Train Acc: 83.6382\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0161\n",
            "Average Test Loss: 0.6661; Test Acc: 77.9736\n",
            "-------------------------------------------\n",
            "Epoch:[17/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4833\n",
            "Average Train Loss: 0.5048; Train Acc: 84.5903\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.1415\n",
            "Average Test Loss: 0.7168; Test Acc: 74.5595\n",
            "-------------------------------------------\n",
            "Epoch:[18/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3071\n",
            "Average Train Loss: 0.4715; Train Acc: 85.3921\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0160\n",
            "Average Test Loss: 0.5007; Test Acc: 84.1410\n",
            "-------------------------------------------\n",
            "Epoch:[19/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.6767\n",
            "Average Train Loss: 0.4624; Train Acc: 85.3170\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0341\n",
            "Average Test Loss: 0.5028; Test Acc: 84.5815\n",
            "-------------------------------------------\n",
            "Epoch:[20/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 3.3033\n",
            "Average Train Loss: 0.4643; Train Acc: 85.6427\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0182\n",
            "Average Test Loss: 0.7313; Test Acc: 74.8899\n",
            "-------------------------------------------\n",
            "Epoch:[21/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2511\n",
            "Average Train Loss: 0.4593; Train Acc: 85.6427\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0101\n",
            "Average Test Loss: 0.4727; Test Acc: 83.2599\n",
            "-------------------------------------------\n",
            "Epoch:[22/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3899\n",
            "Average Train Loss: 0.3940; Train Acc: 87.3465\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0216\n",
            "Average Test Loss: 0.4709; Test Acc: 84.1410\n",
            "-------------------------------------------\n",
            "Epoch:[23/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2922\n",
            "Average Train Loss: 0.3680; Train Acc: 88.0982\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0087\n",
            "Average Test Loss: 0.4120; Test Acc: 87.0044\n",
            "-------------------------------------------\n",
            "Epoch:[24/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.8429\n",
            "Average Train Loss: 0.3331; Train Acc: 89.3510\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0085\n",
            "Average Test Loss: 0.5197; Test Acc: 81.9383\n",
            "-------------------------------------------\n",
            "Epoch:[25/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2525\n",
            "Average Train Loss: 0.3512; Train Acc: 89.0504\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0142\n",
            "Average Test Loss: 0.4231; Test Acc: 84.5815\n",
            "-------------------------------------------\n",
            "Epoch:[26/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1843\n",
            "Average Train Loss: 0.3362; Train Acc: 90.2280\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0081\n",
            "Average Test Loss: 0.4616; Test Acc: 81.0573\n",
            "-------------------------------------------\n",
            "Epoch:[27/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 2.4997\n",
            "Average Train Loss: 0.3347; Train Acc: 89.6517\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0712\n",
            "Average Test Loss: 0.3798; Test Acc: 89.2070\n",
            "-------------------------------------------\n",
            "Epoch:[28/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1938\n",
            "Average Train Loss: 0.3359; Train Acc: 89.1506\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0127\n",
            "Average Test Loss: 0.4194; Test Acc: 86.3436\n",
            "-------------------------------------------\n",
            "Epoch:[29/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1449\n",
            "Average Train Loss: 0.3189; Train Acc: 89.7269\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0082\n",
            "Average Test Loss: 0.3834; Test Acc: 87.9956\n",
            "-------------------------------------------\n",
            "Epoch:[30/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.6824\n",
            "Average Train Loss: 0.3087; Train Acc: 90.5287\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0296\n",
            "Average Test Loss: 0.4316; Test Acc: 87.0044\n",
            "-------------------------------------------\n",
            "Epoch:[31/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.8666\n",
            "Average Train Loss: 0.2935; Train Acc: 90.9045\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0054\n",
            "Average Test Loss: 0.4324; Test Acc: 86.5639\n",
            "-------------------------------------------\n",
            "Epoch:[32/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.6419\n",
            "Average Train Loss: 0.2971; Train Acc: 91.0048\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0095\n",
            "Average Test Loss: 0.3636; Test Acc: 87.7753\n",
            "-------------------------------------------\n",
            "Epoch:[33/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.5471\n",
            "Average Train Loss: 0.3065; Train Acc: 90.1278\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0080\n",
            "Average Test Loss: 0.4216; Test Acc: 85.4626\n",
            "-------------------------------------------\n",
            "Epoch:[34/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.2257\n",
            "Average Train Loss: 0.2908; Train Acc: 90.5537\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0104\n",
            "Average Test Loss: 0.5052; Test Acc: 82.3789\n",
            "-------------------------------------------\n",
            "Epoch:[35/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3830\n",
            "Average Train Loss: 0.2919; Train Acc: 90.3282\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0120\n",
            "Average Test Loss: 0.4707; Test Acc: 86.7841\n",
            "-------------------------------------------\n",
            "Epoch:[36/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.9910\n",
            "Average Train Loss: 0.3012; Train Acc: 90.6289\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0058\n",
            "Average Test Loss: 0.4274; Test Acc: 87.4449\n",
            "-------------------------------------------\n",
            "Epoch:[37/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 3.3086\n",
            "Average Train Loss: 0.2971; Train Acc: 90.5788\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0094\n",
            "Average Test Loss: 0.4164; Test Acc: 87.7753\n",
            "-------------------------------------------\n",
            "Epoch:[38/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4328\n",
            "Average Train Loss: 0.2929; Train Acc: 90.6540\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0048\n",
            "Average Test Loss: 0.4780; Test Acc: 83.5903\n",
            "-------------------------------------------\n",
            "Epoch:[39/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.7276\n",
            "Average Train Loss: 0.2924; Train Acc: 91.0298\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0041\n",
            "Average Test Loss: 0.4664; Test Acc: 83.9207\n",
            "-------------------------------------------\n",
            "Epoch:[40/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3041\n",
            "Average Train Loss: 0.2723; Train Acc: 91.4808\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0038\n",
            "Average Test Loss: 0.4964; Test Acc: 83.8106\n",
            "-------------------------------------------\n",
            "Epoch:[41/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.5381\n",
            "Average Train Loss: 0.2358; Train Acc: 92.1323\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0061\n",
            "Average Test Loss: 0.3491; Test Acc: 88.8767\n",
            "-------------------------------------------\n",
            "Epoch:[42/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.1502\n",
            "Average Train Loss: 0.2446; Train Acc: 92.4330\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0045\n",
            "Average Test Loss: 0.3623; Test Acc: 87.9956\n",
            "-------------------------------------------\n",
            "Epoch:[43/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.6260\n",
            "Average Train Loss: 0.2124; Train Acc: 93.7860\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0041\n",
            "Average Test Loss: 0.3789; Test Acc: 86.4537\n",
            "-------------------------------------------\n",
            "Epoch:[44/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0323\n",
            "Average Train Loss: 0.2218; Train Acc: 92.8339\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0038\n",
            "Average Test Loss: 0.3391; Test Acc: 89.8678\n",
            "-------------------------------------------\n",
            "Epoch:[45/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0288\n",
            "Average Train Loss: 0.2322; Train Acc: 93.2598\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0039\n",
            "Average Test Loss: 0.3707; Test Acc: 86.8943\n",
            "-------------------------------------------\n",
            "Epoch:[46/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3782\n",
            "Average Train Loss: 0.2321; Train Acc: 92.9090\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0038\n",
            "Average Test Loss: 0.3121; Test Acc: 90.0881\n",
            "-------------------------------------------\n",
            "Epoch:[47/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0063\n",
            "Average Train Loss: 0.1990; Train Acc: 93.3601\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0031\n",
            "Average Test Loss: 0.3447; Test Acc: 87.9956\n",
            "-------------------------------------------\n",
            "Epoch:[48/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0710\n",
            "Average Train Loss: 0.1975; Train Acc: 93.5605\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0028\n",
            "Average Test Loss: 0.3425; Test Acc: 89.4273\n",
            "-------------------------------------------\n",
            "Epoch:[49/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0537\n",
            "Average Train Loss: 0.1865; Train Acc: 93.6607\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0031\n",
            "Average Test Loss: 0.3280; Test Acc: 90.3084\n",
            "-------------------------------------------\n",
            "Epoch:[50/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3624\n",
            "Average Train Loss: 0.2055; Train Acc: 93.1596\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0049\n",
            "Average Test Loss: 0.3264; Test Acc: 89.8678\n",
            "-------------------------------------------\n",
            "Epoch:[51/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 1.1437\n",
            "Average Train Loss: 0.2013; Train Acc: 94.1368\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0075\n",
            "Average Test Loss: 0.3419; Test Acc: 89.4273\n",
            "-------------------------------------------\n",
            "Epoch:[52/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1087\n",
            "Average Train Loss: 0.1966; Train Acc: 93.9113\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0038\n",
            "Average Test Loss: 0.3074; Test Acc: 89.8678\n",
            "-------------------------------------------\n",
            "Epoch:[53/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 2.9726\n",
            "Average Train Loss: 0.1910; Train Acc: 94.1118\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0034\n",
            "Average Test Loss: 0.3841; Test Acc: 89.0969\n",
            "-------------------------------------------\n",
            "Epoch:[54/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0649\n",
            "Average Train Loss: 0.1887; Train Acc: 93.8862\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0030\n",
            "Average Test Loss: 0.3283; Test Acc: 89.4273\n",
            "-------------------------------------------\n",
            "Epoch:[55/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.7045\n",
            "Average Train Loss: 0.1927; Train Acc: 93.2348\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0032\n",
            "Average Test Loss: 0.3308; Test Acc: 89.7577\n",
            "-------------------------------------------\n",
            "Epoch:[56/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4074\n",
            "Average Train Loss: 0.1929; Train Acc: 93.7108\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0027\n",
            "Average Test Loss: 0.3446; Test Acc: 88.9868\n",
            "-------------------------------------------\n",
            "Epoch:[57/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0476\n",
            "Average Train Loss: 0.1807; Train Acc: 94.1619\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0025\n",
            "Average Test Loss: 0.3788; Test Acc: 86.7841\n",
            "-------------------------------------------\n",
            "Epoch:[58/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0721\n",
            "Average Train Loss: 0.1808; Train Acc: 93.8862\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0031\n",
            "Average Test Loss: 0.4230; Test Acc: 85.9031\n",
            "-------------------------------------------\n",
            "Epoch:[59/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0187\n",
            "Average Train Loss: 0.1821; Train Acc: 93.7610\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0026\n",
            "Average Test Loss: 0.4035; Test Acc: 86.4537\n",
            "-------------------------------------------\n",
            "Epoch:[60/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.3503\n",
            "Average Train Loss: 0.1776; Train Acc: 94.1869\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0087\n",
            "Average Test Loss: 0.3812; Test Acc: 87.6652\n",
            "-------------------------------------------\n",
            "Epoch:[61/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0923\n",
            "Average Train Loss: 0.1768; Train Acc: 94.2120\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0034\n",
            "Average Test Loss: 0.3536; Test Acc: 87.9956\n",
            "-------------------------------------------\n",
            "Epoch:[62/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.9946\n",
            "Average Train Loss: 0.1573; Train Acc: 94.6379\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0030\n",
            "Average Test Loss: 0.3321; Test Acc: 90.1982\n",
            "-------------------------------------------\n",
            "Epoch:[63/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0728\n",
            "Average Train Loss: 0.1553; Train Acc: 95.0639\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0031\n",
            "Average Test Loss: 0.3468; Test Acc: 88.1057\n",
            "-------------------------------------------\n",
            "Epoch:[64/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2621\n",
            "Average Train Loss: 0.1510; Train Acc: 95.0388\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0036\n",
            "Average Test Loss: 0.3551; Test Acc: 88.8767\n",
            "-------------------------------------------\n",
            "Epoch:[65/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4245\n",
            "Average Train Loss: 0.1520; Train Acc: 94.7632\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0029\n",
            "Average Test Loss: 0.3335; Test Acc: 89.9780\n",
            "-------------------------------------------\n",
            "Epoch:[66/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1465\n",
            "Average Train Loss: 0.1430; Train Acc: 95.4147\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0023\n",
            "Average Test Loss: 0.3113; Test Acc: 90.7489\n",
            "-------------------------------------------\n",
            "Epoch:[67/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.4327\n",
            "Average Train Loss: 0.1382; Train Acc: 95.3145\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0029\n",
            "Average Test Loss: 0.3294; Test Acc: 89.9780\n",
            "-------------------------------------------\n",
            "Epoch:[68/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.8876\n",
            "Average Train Loss: 0.1391; Train Acc: 95.3896\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0025\n",
            "Average Test Loss: 0.3123; Test Acc: 90.3084\n",
            "-------------------------------------------\n",
            "Epoch:[69/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 2.8881\n",
            "Average Train Loss: 0.1634; Train Acc: 95.1140\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0030\n",
            "Average Test Loss: 0.3381; Test Acc: 88.7665\n",
            "-------------------------------------------\n",
            "Epoch:[70/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0096\n",
            "Average Train Loss: 0.1412; Train Acc: 95.2142\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0030\n",
            "Average Test Loss: 0.3286; Test Acc: 89.9780\n",
            "-------------------------------------------\n",
            "Epoch:[71/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2843\n",
            "Average Train Loss: 0.1410; Train Acc: 95.2643\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0027\n",
            "Average Test Loss: 0.3323; Test Acc: 89.7577\n",
            "-------------------------------------------\n",
            "Epoch:[72/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.2187\n",
            "Average Train Loss: 0.1372; Train Acc: 95.1641\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0027\n",
            "Average Test Loss: 0.3371; Test Acc: 88.9868\n",
            "-------------------------------------------\n",
            "Epoch:[73/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 3.7214\n",
            "Average Train Loss: 0.1591; Train Acc: 95.1391\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0030\n",
            "Average Test Loss: 0.4270; Test Acc: 86.7841\n",
            "-------------------------------------------\n",
            "Epoch:[74/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.1627\n",
            "Average Train Loss: 0.1312; Train Acc: 95.6402\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0022\n",
            "Average Test Loss: 0.3428; Test Acc: 88.8767\n",
            "-------------------------------------------\n",
            "Epoch:[75/200]\n",
            "Training...\n",
            "Iter: [250/250] Loss: 0.0478\n",
            "Average Train Loss: 0.1419; Train Acc: 95.2393\n",
            "\n",
            "Testing...\n",
            "Iter: [908/908] Loss: 0.0021\n",
            "Average Test Loss: 0.3366; Test Acc: 90.5286\n",
            "-------------------------------------------\n",
            "Epoch:[76/200]\n",
            "Training...\n",
            "Iter: [212/250] Loss: 0.4309"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-06263f0ea820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cls_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cls_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-47e4b5cf54fa>\u001b[0m in \u001b[0;36mtrain_cls\u001b[0;34m(train_loader, test_loader, network, optimizer, epochs, scheduler)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGEn3ncR_0HV",
        "colab_type": "text"
      },
      "source": [
        "### Report the best test accuracy you can get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv03m-6l_0HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#best test accuracy = 90.5286"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHF6BGsk_0HW",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Segmentation Network\n",
        "In this network, you will use the global features and local features generated by the `Feature` network defined above.\n",
        "\n",
        "The global feature matrix is of size `B x 1024` and the local feature matrix is of size `B x 64 x N`.\n",
        "\n",
        "They need to be stacked together to a new matrix of size `B x 1088 x n` (How?). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jO5a8L-_0HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main train function for classification\n",
        "class Segmentation(nn.Module):\n",
        "    def __init__(self, num_classes, alignment=False):\n",
        "        super(Segmentation, self).__init__()\n",
        "               \n",
        "        self.feature = Feature(alignment=alignment)\n",
        "\n",
        "        ## TASK 2.9\n",
        "        ## shared mlp\n",
        "        ## input size: B x 1088 x N\n",
        "        ## output size: B x num_classes x N\n",
        "        self.shared_mlp = nn.Sequential(  nn.Conv1d(1088, 512, 1),\n",
        "                                          nn.BatchNorm1d(512),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.Conv1d(512, 256, 1),\n",
        "                                          nn.BatchNorm1d(256),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.Conv1d(256, 128, 1),\n",
        "                                          nn.BatchNorm1d(128),\n",
        "                                          nn.ReLU(inplace=True),\n",
        "                                          nn.Conv1d(128, num_classes, 1),\n",
        "                                          nn.LogSoftmax(1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        g, l, trans = self.feature(x)\n",
        "        ## TASK 2.10\n",
        "        # concat global features and local features to a single matrix\n",
        "        # g - B x 1024 , global features -> g - B x 1024 , N\n",
        "        # l - B x 64 x N, local features\n",
        "        # x - B x 1088 x N, concatenated features\n",
        "        x = torch.cat([g.view(-1, 1024, 1).repeat(1, 1, x.shape[-1]), l], 1)\n",
        "            \n",
        "        ## TASK 2.9\n",
        "        ## forward of shared mlp\n",
        "        # input - B x 1088 x N\n",
        "        # output - B x num_classes x N  \n",
        "        x = self.shared_mlp(x)\n",
        "        \n",
        "        return x, trans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_B77Whq_0HX",
        "colab_type": "code",
        "outputId": "9767c69e-25e2-4c30-d385-59372efe54e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "Segmentation(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Segmentation(\n",
              "  (feature): Feature(\n",
              "    (local_nn): Sequential(\n",
              "      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (global_nn): Sequential(\n",
              "      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "      (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (shared_mlp): Sequential(\n",
              "    (0): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
              "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
              "    (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv1d(128, 5, kernel_size=(1,), stride=(1,))\n",
              "    (10): LogSoftmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGTM0EkL_0HY",
        "colab_type": "text"
      },
      "source": [
        "### 2.5.1 Calculating Intersection over Union (IoU) \n",
        "For 2D image, the IoU is calculated as follows,\n",
        "![iou](img/iou.png)\n",
        "\n",
        "How is it used in the literature of point clouds?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ej9Fti_0HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TASK 2.11\n",
        "# implement the helper functions to calculate the IoU\n",
        "def get_i_and_u(pred, target, num_classes):\n",
        "    \"\"\"Calculate intersection and union between pred and target.\n",
        "    \n",
        "    pred -- B x N matrix\n",
        "    target -- B x N matrix\n",
        "    num_classes -- number of classes\n",
        "    \n",
        "    return i, u\n",
        "    i -- B x N binary matrix, intersection, i[b, n] equals 1 if and only if it is a true-positive.\n",
        "    u -- B x N binary matrix, union, u[b, n] equals 0 if and only if it is a true-negative\n",
        "    \"\"\"\n",
        "    ## TASK 2.11\n",
        "    ## calculate i and u here\n",
        "    ## hint: useful function `F.one_hot`    \n",
        "    ## hint: use element-wise logical tensor operation (`&` and `|`)\n",
        "    iou=0\n",
        "    for c in range(1, num_classes+1):\n",
        "      p = (pred == c)\n",
        "      t = (target == c)\n",
        "      i = (p & t).sum()\n",
        "      u = (p | t).sum()\n",
        "      iou += 1 if u == 0 else i.float()/u.float()\n",
        "    return iou/num_classes\n",
        "\n",
        "def get_iou(pred, target, num_classes):\n",
        "    \"\"\"Calculate IoU\n",
        "    pred -- B x N matrix\n",
        "    target -- B x N matrix\n",
        "    num_classes -- number of classes\n",
        "    \n",
        "    return iou\n",
        "    iou -- B matrix, iou[b] is the IoU of b-th point cloud in this batch\n",
        "    \"\"\"\n",
        "\n",
        "    ## use the helper function `i_and_u` defined above\n",
        "    return torch.Tensor([get_i_and_u(pred[j], target[j], num_classes) for j in range(pred.shape[0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbdUwyxv_0HZ",
        "colab_type": "text"
      },
      "source": [
        "### 2.5.2 Train this network on ShapeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2-N7dCo_0HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main train function for segmentation\n",
        "def train_seg(train_loader, test_loader, network, optimizer, epochs, scheduler):  \n",
        "    reg = OrthoLoss()\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
        "        print('Training...')\n",
        "        network.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        ious = []\n",
        "        for batch, (pos, label) in enumerate(train_loader):\n",
        "            network.zero_grad()\n",
        "            pos, label = pos.cuda(), label.cuda()\n",
        "            ## TASK 2.12\n",
        "            ## forward propagation\n",
        "            output, trans = network(pos)\n",
        "            loss = F.nll_loss(output, label)\n",
        "            ##########\n",
        "            if trans is not None:\n",
        "                loss += reg(trans) * 0.001        \n",
        "\n",
        "            pred = output.max(1)[1]\n",
        "            correct += pred.eq(label).sum().item()\n",
        "            total += label.numel()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            #ref_iou(pred, label, train_loader.dataset.num_classes)\n",
        "            ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
        "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
        "            \n",
        "        \n",
        "        scheduler.step()\n",
        "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}; Train mean IoU: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
        "\n",
        "        print('\\nTesting...')\n",
        "        with torch.no_grad():\n",
        "            network.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            ious = []\n",
        "            for batch, (pos, label) in enumerate(test_loader):\n",
        "                pos, label = pos.cuda(), label.cuda()\n",
        "                \n",
        "                ## TASK 2.12\n",
        "                ## forward propagation\n",
        "                output, trans = network(pos)\n",
        "                loss = F.nll_loss(output, label)\n",
        "                ##########\n",
        "                \n",
        "                if trans is not None:\n",
        "                    loss += reg(trans) * 0.001   \n",
        "\n",
        "                pred = output.max(1)[1]\n",
        "                correct += pred.eq(label).sum().item()\n",
        "                total += label.numel()\n",
        "\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
        "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
        "\n",
        "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}; Test mean IoU: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
        "        print('-------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ9sGBjh_0Ha",
        "colab_type": "code",
        "outputId": "563b4d7e-c8db-4f6f-fa11-00324580f455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network = Segmentation(train_seg_dataset.num_classes, alignment=True).cuda()\n",
        "epochs = 200 # you can change the value to a small number for debugging\n",
        "\n",
        "## TASK 2.13\n",
        "# see Appendix C\n",
        "# choose an optimizer and an initial learning rate\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
        "# choose a lr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.5)\n",
        "#######3\n",
        "\n",
        "train_seg(train_seg_loader, test_seg_loader, network, optimizer, epochs, scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:[01/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.5216\n",
            "Average Train Loss: 0.6353; Train Acc: 78.0845; Train mean IoU: 0.6929\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.5050\n",
            "Average Test Loss: 0.5107; Test Acc: 81.9947; Test mean IoU: 0.7211\n",
            "-------------------------------------------\n",
            "Epoch:[02/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.5372\n",
            "Average Train Loss: 0.4110; Train Acc: 85.9453; Train mean IoU: 0.7814\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3375\n",
            "Average Test Loss: 0.4639; Test Acc: 84.0452; Test mean IoU: 0.7427\n",
            "-------------------------------------------\n",
            "Epoch:[03/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.3523\n",
            "Average Train Loss: 0.3552; Train Acc: 87.5756; Train mean IoU: 0.8050\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3499\n",
            "Average Test Loss: 0.3846; Test Acc: 86.3953; Test mean IoU: 0.7947\n",
            "-------------------------------------------\n",
            "Epoch:[04/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.3056\n",
            "Average Train Loss: 0.3188; Train Acc: 88.6227; Train mean IoU: 0.8184\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2985\n",
            "Average Test Loss: 0.4544; Test Acc: 83.8788; Test mean IoU: 0.7581\n",
            "-------------------------------------------\n",
            "Epoch:[05/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.3710\n",
            "Average Train Loss: 0.3099; Train Acc: 88.8077; Train mean IoU: 0.8226\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2425\n",
            "Average Test Loss: 0.3800; Test Acc: 86.5015; Test mean IoU: 0.7765\n",
            "-------------------------------------------\n",
            "Epoch:[06/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2820\n",
            "Average Train Loss: 0.2867; Train Acc: 89.4210; Train mean IoU: 0.8288\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.4768\n",
            "Average Test Loss: 0.4480; Test Acc: 82.4993; Test mean IoU: 0.7247\n",
            "-------------------------------------------\n",
            "Epoch:[07/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2806\n",
            "Average Train Loss: 0.2896; Train Acc: 89.2451; Train mean IoU: 0.8266\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3379\n",
            "Average Test Loss: 0.5529; Test Acc: 80.7617; Test mean IoU: 0.7210\n",
            "-------------------------------------------\n",
            "Epoch:[08/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.3947\n",
            "Average Train Loss: 0.2736; Train Acc: 89.7211; Train mean IoU: 0.8338\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3163\n",
            "Average Test Loss: 0.3328; Test Acc: 87.3681; Test mean IoU: 0.7993\n",
            "-------------------------------------------\n",
            "Epoch:[09/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2356\n",
            "Average Train Loss: 0.2635; Train Acc: 90.0456; Train mean IoU: 0.8382\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3028\n",
            "Average Test Loss: 0.3672; Test Acc: 85.3886; Test mean IoU: 0.7758\n",
            "-------------------------------------------\n",
            "Epoch:[10/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.3484\n",
            "Average Train Loss: 0.2631; Train Acc: 90.0133; Train mean IoU: 0.8397\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.4976\n",
            "Average Test Loss: 0.3872; Test Acc: 84.7095; Test mean IoU: 0.7980\n",
            "-------------------------------------------\n",
            "Epoch:[11/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2034\n",
            "Average Train Loss: 0.2569; Train Acc: 90.2264; Train mean IoU: 0.8417\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2693\n",
            "Average Test Loss: 0.4074; Test Acc: 83.9260; Test mean IoU: 0.7669\n",
            "-------------------------------------------\n",
            "Epoch:[12/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2221\n",
            "Average Train Loss: 0.2483; Train Acc: 90.4675; Train mean IoU: 0.8449\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3558\n",
            "Average Test Loss: 0.4342; Test Acc: 82.4925; Test mean IoU: 0.7677\n",
            "-------------------------------------------\n",
            "Epoch:[13/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2936\n",
            "Average Train Loss: 0.2443; Train Acc: 90.5550; Train mean IoU: 0.8450\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2293\n",
            "Average Test Loss: 0.3131; Test Acc: 87.9081; Test mean IoU: 0.8170\n",
            "-------------------------------------------\n",
            "Epoch:[14/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2630\n",
            "Average Train Loss: 0.2344; Train Acc: 90.9034; Train mean IoU: 0.8492\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3666\n",
            "Average Test Loss: 0.3562; Test Acc: 85.6248; Test mean IoU: 0.8023\n",
            "-------------------------------------------\n",
            "Epoch:[15/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2199\n",
            "Average Train Loss: 0.2303; Train Acc: 91.0290; Train mean IoU: 0.8508\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2826\n",
            "Average Test Loss: 0.3062; Test Acc: 88.2320; Test mean IoU: 0.8045\n",
            "-------------------------------------------\n",
            "Epoch:[16/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1969\n",
            "Average Train Loss: 0.2209; Train Acc: 91.4130; Train mean IoU: 0.8590\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3661\n",
            "Average Test Loss: 0.6143; Test Acc: 76.5280; Test mean IoU: 0.6892\n",
            "-------------------------------------------\n",
            "Epoch:[17/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1885\n",
            "Average Train Loss: 0.2179; Train Acc: 91.5040; Train mean IoU: 0.8589\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3219\n",
            "Average Test Loss: 0.3182; Test Acc: 87.3133; Test mean IoU: 0.8181\n",
            "-------------------------------------------\n",
            "Epoch:[18/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2622\n",
            "Average Train Loss: 0.2152; Train Acc: 91.6395; Train mean IoU: 0.8611\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3269\n",
            "Average Test Loss: 0.4322; Test Acc: 83.2975; Test mean IoU: 0.7705\n",
            "-------------------------------------------\n",
            "Epoch:[19/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2060\n",
            "Average Train Loss: 0.2092; Train Acc: 91.7800; Train mean IoU: 0.8638\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1869\n",
            "Average Test Loss: 0.3055; Test Acc: 87.6668; Test mean IoU: 0.8145\n",
            "-------------------------------------------\n",
            "Epoch:[20/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2283\n",
            "Average Train Loss: 0.2096; Train Acc: 91.7913; Train mean IoU: 0.8647\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3688\n",
            "Average Test Loss: 0.8527; Test Acc: 73.4163; Test mean IoU: 0.6514\n",
            "-------------------------------------------\n",
            "Epoch:[21/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2142\n",
            "Average Train Loss: 0.2072; Train Acc: 91.9597; Train mean IoU: 0.8658\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2171\n",
            "Average Test Loss: 0.2743; Test Acc: 89.5864; Test mean IoU: 0.8370\n",
            "-------------------------------------------\n",
            "Epoch:[22/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2451\n",
            "Average Train Loss: 0.1932; Train Acc: 92.4491; Train mean IoU: 0.8753\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1791\n",
            "Average Test Loss: 0.2906; Test Acc: 89.0994; Test mean IoU: 0.8348\n",
            "-------------------------------------------\n",
            "Epoch:[23/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1324\n",
            "Average Train Loss: 0.1880; Train Acc: 92.6266; Train mean IoU: 0.8772\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1845\n",
            "Average Test Loss: 0.2858; Test Acc: 89.3515; Test mean IoU: 0.8426\n",
            "-------------------------------------------\n",
            "Epoch:[24/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1909\n",
            "Average Train Loss: 0.1822; Train Acc: 92.8489; Train mean IoU: 0.8815\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1813\n",
            "Average Test Loss: 0.3106; Test Acc: 88.4605; Test mean IoU: 0.8214\n",
            "-------------------------------------------\n",
            "Epoch:[25/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1544\n",
            "Average Train Loss: 0.1816; Train Acc: 92.8158; Train mean IoU: 0.8814\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.4011\n",
            "Average Test Loss: 0.6263; Test Acc: 78.4973; Test mean IoU: 0.7267\n",
            "-------------------------------------------\n",
            "Epoch:[26/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1477\n",
            "Average Train Loss: 0.1791; Train Acc: 92.9592; Train mean IoU: 0.8838\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1736\n",
            "Average Test Loss: 0.3544; Test Acc: 87.6535; Test mean IoU: 0.8180\n",
            "-------------------------------------------\n",
            "Epoch:[27/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1544\n",
            "Average Train Loss: 0.1753; Train Acc: 93.1371; Train mean IoU: 0.8860\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1552\n",
            "Average Test Loss: 0.3206; Test Acc: 87.4701; Test mean IoU: 0.8078\n",
            "-------------------------------------------\n",
            "Epoch:[28/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2207\n",
            "Average Train Loss: 0.1728; Train Acc: 93.1946; Train mean IoU: 0.8868\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1917\n",
            "Average Test Loss: 0.3601; Test Acc: 86.9105; Test mean IoU: 0.7963\n",
            "-------------------------------------------\n",
            "Epoch:[29/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1467\n",
            "Average Train Loss: 0.1715; Train Acc: 93.2438; Train mean IoU: 0.8871\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1653\n",
            "Average Test Loss: 0.3210; Test Acc: 88.8264; Test mean IoU: 0.8305\n",
            "-------------------------------------------\n",
            "Epoch:[30/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1872\n",
            "Average Train Loss: 0.1701; Train Acc: 93.2626; Train mean IoU: 0.8879\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2233\n",
            "Average Test Loss: 0.5428; Test Acc: 82.5108; Test mean IoU: 0.7597\n",
            "-------------------------------------------\n",
            "Epoch:[31/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1598\n",
            "Average Train Loss: 0.1692; Train Acc: 93.3174; Train mean IoU: 0.8890\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1775\n",
            "Average Test Loss: 0.3179; Test Acc: 87.4202; Test mean IoU: 0.8138\n",
            "-------------------------------------------\n",
            "Epoch:[32/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1597\n",
            "Average Train Loss: 0.1650; Train Acc: 93.4782; Train mean IoU: 0.8918\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1574\n",
            "Average Test Loss: 0.3483; Test Acc: 86.5751; Test mean IoU: 0.7962\n",
            "-------------------------------------------\n",
            "Epoch:[33/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1704\n",
            "Average Train Loss: 0.1644; Train Acc: 93.4731; Train mean IoU: 0.8907\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2612\n",
            "Average Test Loss: 0.3482; Test Acc: 87.1505; Test mean IoU: 0.8148\n",
            "-------------------------------------------\n",
            "Epoch:[34/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1523\n",
            "Average Train Loss: 0.1612; Train Acc: 93.6122; Train mean IoU: 0.8926\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1670\n",
            "Average Test Loss: 0.3198; Test Acc: 88.8247; Test mean IoU: 0.8324\n",
            "-------------------------------------------\n",
            "Epoch:[35/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1446\n",
            "Average Train Loss: 0.1595; Train Acc: 93.6641; Train mean IoU: 0.8934\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3376\n",
            "Average Test Loss: 0.5593; Test Acc: 82.3039; Test mean IoU: 0.7617\n",
            "-------------------------------------------\n",
            "Epoch:[36/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1389\n",
            "Average Train Loss: 0.1577; Train Acc: 93.7525; Train mean IoU: 0.8952\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2843\n",
            "Average Test Loss: 0.3100; Test Acc: 88.8738; Test mean IoU: 0.8202\n",
            "-------------------------------------------\n",
            "Epoch:[37/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2068\n",
            "Average Train Loss: 0.1563; Train Acc: 93.7718; Train mean IoU: 0.8953\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1898\n",
            "Average Test Loss: 0.3424; Test Acc: 87.7947; Test mean IoU: 0.8202\n",
            "-------------------------------------------\n",
            "Epoch:[38/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1888\n",
            "Average Train Loss: 0.1548; Train Acc: 93.8026; Train mean IoU: 0.8962\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3299\n",
            "Average Test Loss: 0.6641; Test Acc: 80.2865; Test mean IoU: 0.7220\n",
            "-------------------------------------------\n",
            "Epoch:[39/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1418\n",
            "Average Train Loss: 0.1553; Train Acc: 93.8315; Train mean IoU: 0.8968\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.4931\n",
            "Average Test Loss: 0.6398; Test Acc: 80.6430; Test mean IoU: 0.7012\n",
            "-------------------------------------------\n",
            "Epoch:[40/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1814\n",
            "Average Train Loss: 0.1536; Train Acc: 93.8825; Train mean IoU: 0.8977\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3830\n",
            "Average Test Loss: 0.8287; Test Acc: 75.3533; Test mean IoU: 0.6864\n",
            "-------------------------------------------\n",
            "Epoch:[41/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1471\n",
            "Average Train Loss: 0.1448; Train Acc: 94.2713; Train mean IoU: 0.9037\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1681\n",
            "Average Test Loss: 0.3139; Test Acc: 88.8751; Test mean IoU: 0.8352\n",
            "-------------------------------------------\n",
            "Epoch:[42/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1387\n",
            "Average Train Loss: 0.1410; Train Acc: 94.3840; Train mean IoU: 0.9051\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1425\n",
            "Average Test Loss: 0.3236; Test Acc: 88.4474; Test mean IoU: 0.8304\n",
            "-------------------------------------------\n",
            "Epoch:[43/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1464\n",
            "Average Train Loss: 0.1392; Train Acc: 94.4590; Train mean IoU: 0.9068\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1403\n",
            "Average Test Loss: 0.2881; Test Acc: 90.2183; Test mean IoU: 0.8541\n",
            "-------------------------------------------\n",
            "Epoch:[44/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1251\n",
            "Average Train Loss: 0.1389; Train Acc: 94.4781; Train mean IoU: 0.9065\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1228\n",
            "Average Test Loss: 0.3120; Test Acc: 89.1700; Test mean IoU: 0.8336\n",
            "-------------------------------------------\n",
            "Epoch:[45/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1382\n",
            "Average Train Loss: 0.1366; Train Acc: 94.5872; Train mean IoU: 0.9083\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1998\n",
            "Average Test Loss: 0.3060; Test Acc: 89.1609; Test mean IoU: 0.8392\n",
            "-------------------------------------------\n",
            "Epoch:[46/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2174\n",
            "Average Train Loss: 0.1465; Train Acc: 94.2081; Train mean IoU: 0.9022\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.3044\n",
            "Average Test Loss: 0.3946; Test Acc: 85.2304; Test mean IoU: 0.7868\n",
            "-------------------------------------------\n",
            "Epoch:[47/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1294\n",
            "Average Train Loss: 0.1485; Train Acc: 94.0584; Train mean IoU: 0.9005\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1634\n",
            "Average Test Loss: 0.3742; Test Acc: 87.5833; Test mean IoU: 0.8163\n",
            "-------------------------------------------\n",
            "Epoch:[48/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1660\n",
            "Average Train Loss: 0.1381; Train Acc: 94.4865; Train mean IoU: 0.9068\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1436\n",
            "Average Test Loss: 0.2970; Test Acc: 89.8267; Test mean IoU: 0.8389\n",
            "-------------------------------------------\n",
            "Epoch:[49/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1590\n",
            "Average Train Loss: 0.1374; Train Acc: 94.5057; Train mean IoU: 0.9065\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1559\n",
            "Average Test Loss: 0.3481; Test Acc: 87.7191; Test mean IoU: 0.8208\n",
            "-------------------------------------------\n",
            "Epoch:[50/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1574\n",
            "Average Train Loss: 0.1351; Train Acc: 94.5918; Train mean IoU: 0.9080\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1506\n",
            "Average Test Loss: 0.3391; Test Acc: 88.0983; Test mean IoU: 0.8207\n",
            "-------------------------------------------\n",
            "Epoch:[51/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1338\n",
            "Average Train Loss: 0.1343; Train Acc: 94.6146; Train mean IoU: 0.9089\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1550\n",
            "Average Test Loss: 0.3155; Test Acc: 89.3154; Test mean IoU: 0.8311\n",
            "-------------------------------------------\n",
            "Epoch:[52/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1312\n",
            "Average Train Loss: 0.1326; Train Acc: 94.6874; Train mean IoU: 0.9098\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1616\n",
            "Average Test Loss: 0.2866; Test Acc: 90.4148; Test mean IoU: 0.8560\n",
            "-------------------------------------------\n",
            "Epoch:[53/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1114\n",
            "Average Train Loss: 0.1322; Train Acc: 94.7226; Train mean IoU: 0.9104\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1602\n",
            "Average Test Loss: 0.3973; Test Acc: 87.3717; Test mean IoU: 0.8110\n",
            "-------------------------------------------\n",
            "Epoch:[54/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1432\n",
            "Average Train Loss: 0.1307; Train Acc: 94.7687; Train mean IoU: 0.9111\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2695\n",
            "Average Test Loss: 0.3336; Test Acc: 88.8973; Test mean IoU: 0.8327\n",
            "-------------------------------------------\n",
            "Epoch:[55/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1059\n",
            "Average Train Loss: 0.1321; Train Acc: 94.6943; Train mean IoU: 0.9099\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1835\n",
            "Average Test Loss: 0.4017; Test Acc: 87.7596; Test mean IoU: 0.8147\n",
            "-------------------------------------------\n",
            "Epoch:[56/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.2158\n",
            "Average Train Loss: 0.1303; Train Acc: 94.7726; Train mean IoU: 0.9113\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2907\n",
            "Average Test Loss: 0.4013; Test Acc: 86.2167; Test mean IoU: 0.7811\n",
            "-------------------------------------------\n",
            "Epoch:[57/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1387\n",
            "Average Train Loss: 0.1287; Train Acc: 94.8294; Train mean IoU: 0.9123\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1411\n",
            "Average Test Loss: 0.3074; Test Acc: 89.3390; Test mean IoU: 0.8321\n",
            "-------------------------------------------\n",
            "Epoch:[58/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1288\n",
            "Average Train Loss: 0.1287; Train Acc: 94.8481; Train mean IoU: 0.9124\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2047\n",
            "Average Test Loss: 0.3215; Test Acc: 88.6903; Test mean IoU: 0.8351\n",
            "-------------------------------------------\n",
            "Epoch:[59/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1449\n",
            "Average Train Loss: 0.1280; Train Acc: 94.8670; Train mean IoU: 0.9127\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2445\n",
            "Average Test Loss: 0.3879; Test Acc: 86.4311; Test mean IoU: 0.7954\n",
            "-------------------------------------------\n",
            "Epoch:[60/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1158\n",
            "Average Train Loss: 0.1288; Train Acc: 94.8081; Train mean IoU: 0.9122\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1905\n",
            "Average Test Loss: 0.4492; Test Acc: 85.3133; Test mean IoU: 0.7764\n",
            "-------------------------------------------\n",
            "Epoch:[61/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1772\n",
            "Average Train Loss: 0.1239; Train Acc: 95.0161; Train mean IoU: 0.9152\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1145\n",
            "Average Test Loss: 0.2886; Test Acc: 90.7774; Test mean IoU: 0.8578\n",
            "-------------------------------------------\n",
            "Epoch:[62/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1234\n",
            "Average Train Loss: 0.1214; Train Acc: 95.1209; Train mean IoU: 0.9171\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1137\n",
            "Average Test Loss: 0.2949; Test Acc: 90.3780; Test mean IoU: 0.8532\n",
            "-------------------------------------------\n",
            "Epoch:[63/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1093\n",
            "Average Train Loss: 0.1207; Train Acc: 95.1497; Train mean IoU: 0.9166\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1371\n",
            "Average Test Loss: 0.3148; Test Acc: 89.5064; Test mean IoU: 0.8414\n",
            "-------------------------------------------\n",
            "Epoch:[64/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1290\n",
            "Average Train Loss: 0.1197; Train Acc: 95.2123; Train mean IoU: 0.9181\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1802\n",
            "Average Test Loss: 0.3520; Test Acc: 88.8862; Test mean IoU: 0.8330\n",
            "-------------------------------------------\n",
            "Epoch:[65/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.0957\n",
            "Average Train Loss: 0.1188; Train Acc: 95.2260; Train mean IoU: 0.9183\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1798\n",
            "Average Test Loss: 0.3648; Test Acc: 88.6044; Test mean IoU: 0.8223\n",
            "-------------------------------------------\n",
            "Epoch:[66/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1437\n",
            "Average Train Loss: 0.1193; Train Acc: 95.2100; Train mean IoU: 0.9179\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2345\n",
            "Average Test Loss: 0.3054; Test Acc: 90.5165; Test mean IoU: 0.8591\n",
            "-------------------------------------------\n",
            "Epoch:[67/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1213\n",
            "Average Train Loss: 0.1201; Train Acc: 95.1651; Train mean IoU: 0.9180\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1954\n",
            "Average Test Loss: 0.3164; Test Acc: 90.1758; Test mean IoU: 0.8493\n",
            "-------------------------------------------\n",
            "Epoch:[68/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1066\n",
            "Average Train Loss: 0.1186; Train Acc: 95.2132; Train mean IoU: 0.9182\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2035\n",
            "Average Test Loss: 0.2996; Test Acc: 90.6340; Test mean IoU: 0.8573\n",
            "-------------------------------------------\n",
            "Epoch:[69/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1450\n",
            "Average Train Loss: 0.1179; Train Acc: 95.2389; Train mean IoU: 0.9180\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1734\n",
            "Average Test Loss: 0.3057; Test Acc: 89.9984; Test mean IoU: 0.8504\n",
            "-------------------------------------------\n",
            "Epoch:[70/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1302\n",
            "Average Train Loss: 0.1181; Train Acc: 95.2501; Train mean IoU: 0.9187\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2502\n",
            "Average Test Loss: 0.3756; Test Acc: 88.3288; Test mean IoU: 0.8355\n",
            "-------------------------------------------\n",
            "Epoch:[71/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1146\n",
            "Average Train Loss: 0.1170; Train Acc: 95.2791; Train mean IoU: 0.9192\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1431\n",
            "Average Test Loss: 0.3156; Test Acc: 89.7915; Test mean IoU: 0.8428\n",
            "-------------------------------------------\n",
            "Epoch:[72/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1423\n",
            "Average Train Loss: 0.1174; Train Acc: 95.2536; Train mean IoU: 0.9185\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1848\n",
            "Average Test Loss: 0.3245; Test Acc: 90.0180; Test mean IoU: 0.8473\n",
            "-------------------------------------------\n",
            "Epoch:[73/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1413\n",
            "Average Train Loss: 0.1164; Train Acc: 95.2991; Train mean IoU: 0.9193\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1196\n",
            "Average Test Loss: 0.3184; Test Acc: 90.2869; Test mean IoU: 0.8450\n",
            "-------------------------------------------\n",
            "Epoch:[74/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1398\n",
            "Average Train Loss: 0.1157; Train Acc: 95.3243; Train mean IoU: 0.9194\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1822\n",
            "Average Test Loss: 0.3076; Test Acc: 90.5699; Test mean IoU: 0.8495\n",
            "-------------------------------------------\n",
            "Epoch:[75/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1383\n",
            "Average Train Loss: 0.1155; Train Acc: 95.3358; Train mean IoU: 0.9199\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1908\n",
            "Average Test Loss: 0.3216; Test Acc: 89.9440; Test mean IoU: 0.8468\n",
            "-------------------------------------------\n",
            "Epoch:[76/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1411\n",
            "Average Train Loss: 0.1155; Train Acc: 95.3407; Train mean IoU: 0.9195\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1572\n",
            "Average Test Loss: 0.3198; Test Acc: 89.9527; Test mean IoU: 0.8510\n",
            "-------------------------------------------\n",
            "Epoch:[77/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1291\n",
            "Average Train Loss: 0.1149; Train Acc: 95.3532; Train mean IoU: 0.9204\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1454\n",
            "Average Test Loss: 0.3129; Test Acc: 90.1022; Test mean IoU: 0.8478\n",
            "-------------------------------------------\n",
            "Epoch:[78/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1119\n",
            "Average Train Loss: 0.1141; Train Acc: 95.3916; Train mean IoU: 0.9203\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1993\n",
            "Average Test Loss: 0.3203; Test Acc: 89.8386; Test mean IoU: 0.8446\n",
            "-------------------------------------------\n",
            "Epoch:[79/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1171\n",
            "Average Train Loss: 0.1138; Train Acc: 95.4094; Train mean IoU: 0.9212\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2823\n",
            "Average Test Loss: 0.3271; Test Acc: 89.8343; Test mean IoU: 0.8542\n",
            "-------------------------------------------\n",
            "Epoch:[80/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1244\n",
            "Average Train Loss: 0.1130; Train Acc: 95.4272; Train mean IoU: 0.9218\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1835\n",
            "Average Test Loss: 0.3384; Test Acc: 89.7454; Test mean IoU: 0.8505\n",
            "-------------------------------------------\n",
            "Epoch:[81/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1047\n",
            "Average Train Loss: 0.1111; Train Acc: 95.4961; Train mean IoU: 0.9223\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2198\n",
            "Average Test Loss: 0.3214; Test Acc: 89.7796; Test mean IoU: 0.8507\n",
            "-------------------------------------------\n",
            "Epoch:[82/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1038\n",
            "Average Train Loss: 0.1103; Train Acc: 95.5500; Train mean IoU: 0.9232\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1764\n",
            "Average Test Loss: 0.3092; Test Acc: 90.6503; Test mean IoU: 0.8595\n",
            "-------------------------------------------\n",
            "Epoch:[83/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1236\n",
            "Average Train Loss: 0.1102; Train Acc: 95.5411; Train mean IoU: 0.9232\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2550\n",
            "Average Test Loss: 0.3133; Test Acc: 90.4450; Test mean IoU: 0.8562\n",
            "-------------------------------------------\n",
            "Epoch:[84/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1264\n",
            "Average Train Loss: 0.1100; Train Acc: 95.5512; Train mean IoU: 0.9233\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1767\n",
            "Average Test Loss: 0.3111; Test Acc: 90.5905; Test mean IoU: 0.8594\n",
            "-------------------------------------------\n",
            "Epoch:[85/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.0941\n",
            "Average Train Loss: 0.1091; Train Acc: 95.5795; Train mean IoU: 0.9240\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1651\n",
            "Average Test Loss: 0.3157; Test Acc: 90.7344; Test mean IoU: 0.8578\n",
            "-------------------------------------------\n",
            "Epoch:[86/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1008\n",
            "Average Train Loss: 0.1089; Train Acc: 95.5809; Train mean IoU: 0.9235\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2362\n",
            "Average Test Loss: 0.3140; Test Acc: 90.4758; Test mean IoU: 0.8537\n",
            "-------------------------------------------\n",
            "Epoch:[87/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1201\n",
            "Average Train Loss: 0.1094; Train Acc: 95.5736; Train mean IoU: 0.9235\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2436\n",
            "Average Test Loss: 0.3085; Test Acc: 90.8290; Test mean IoU: 0.8605\n",
            "-------------------------------------------\n",
            "Epoch:[88/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1026\n",
            "Average Train Loss: 0.1095; Train Acc: 95.5729; Train mean IoU: 0.9234\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1680\n",
            "Average Test Loss: 0.3296; Test Acc: 90.1654; Test mean IoU: 0.8472\n",
            "-------------------------------------------\n",
            "Epoch:[89/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1104\n",
            "Average Train Loss: 0.1085; Train Acc: 95.5949; Train mean IoU: 0.9242\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2391\n",
            "Average Test Loss: 0.3091; Test Acc: 90.9336; Test mean IoU: 0.8577\n",
            "-------------------------------------------\n",
            "Epoch:[90/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.0879\n",
            "Average Train Loss: 0.1078; Train Acc: 95.6447; Train mean IoU: 0.9245\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1710\n",
            "Average Test Loss: 0.3105; Test Acc: 90.7247; Test mean IoU: 0.8546\n",
            "-------------------------------------------\n",
            "Epoch:[91/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1147\n",
            "Average Train Loss: 0.1087; Train Acc: 95.5803; Train mean IoU: 0.9238\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.2685\n",
            "Average Test Loss: 0.3102; Test Acc: 90.7553; Test mean IoU: 0.8592\n",
            "-------------------------------------------\n",
            "Epoch:[92/200]\n",
            "Training...\n",
            "Iter: [147/147] Loss: 0.1009\n",
            "Average Train Loss: 0.1075; Train Acc: 95.6331; Train mean IoU: 0.9244\n",
            "\n",
            "Testing...\n",
            "Iter: [341/341] Loss: 0.1746\n",
            "Average Test Loss: 0.3093; Test Acc: 90.6738; Test mean IoU: 0.8557\n",
            "-------------------------------------------\n",
            "Epoch:[93/200]\n",
            "Training...\n",
            "Iter: [144/147] Loss: 0.0933"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b104c281a801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#######3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_seg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seg_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seg_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-cfd3d6e53eea>\u001b[0m in \u001b[0;36mtrain_seg\u001b[0;34m(train_loader, test_loader, network, optimizer, epochs, scheduler)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmUxNXx_0Hb",
        "colab_type": "text"
      },
      "source": [
        "### Report the best test mIoU you can get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVQremOi_0Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best test mIoU = 0.8605"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbZbhPm5oAyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}